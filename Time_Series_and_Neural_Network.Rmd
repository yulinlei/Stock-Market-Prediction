---
title: "Stock Price"
output: pdf_document
---


Stock prices are rather difficult to forecast. Proponents of efficient market hypotheses arguet hat stock prices cannot be predicted since the market prices have already reflected all known and expected fundamentals. However, precisely because the current market price reflect all relevant information, and therefore the current and historical prices are extremely useful for predicting the future prices since some relevant information is entering the stock markets continuously. It is well documented evidence that stock prices demonstrate strong momentum, short-term continuation and long-term reversal, and other identifiable patterns. 





**Time seris Analysis: ARIMA, SARIMA and Spectral Analysis**\newline

A time series is a sequence of vectors, x(t), t = 0,1,â€¦ , where t represents elapsed time.
For simplicity we will consider here only sequences of scalars, although the techniques considered generalise readily to vector series. Theoretically, x may be a value which varies continuously with t, such as a Closing price. In practice, for any given physical system, x will be sampled to give a series of discrete data points, equally spaced in time. The rate at which samples are taken dictates the maximum resolution of the model; however, it is not always the case that the model with the highest resolution has the best predictive power, so that superior results may be obtained by employing only every nth point in the series. 

Time series are generally sequences of measurements of one or more visible variables of an
underlying dynamic system, whose state changes with time as a function of its current state vector$u(t+1) = F(u(t))$. Such dynamic systems may evolve over time to an attracting set of points that is regular and of simple shape; any time series derived from such a system would also have a smooth and regular appearance. However another result is possible: the system may evolve to a chaotic attractor. Here, the path of the state vector through the attractor is non-periodic and because of this any time series derived from it will have a complex appearance and behaviour. In a real-world system such as a stock market, the nature and structure of the state space is obscure; so that the actual variables that contribute to the state vector are unknown or debatable. The task for a time series predictor can therefore be rephrased: given measurements of one component of the state vector of a dynamic system is it possible to reconstruct the (possibly) chaotic dynamics of the phase space and thereby predict the evolution of the measured variable? 


```{r,echo=FALSE,warning=FALSE}
suppressPackageStartupMessages(library(tseries))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(xts))
suppressPackageStartupMessages(library(dygraphs))
suppressPackageStartupMessages(library("forecast"))
suppressPackageStartupMessages(library("TSA"))
suppressPackageStartupMessages(library("astsa"))
suppressPackageStartupMessages(library(neuralnet))
```


```{r,echo=FALSE,warning=FALSE}
apple = as.data.frame(get.hist.quote(instrument='AAPL',start='2014-09-19',end='2016-03-31'))
apple$Date = as.Date(rownames(apple))
facebook = as.data.frame(get.hist.quote(instrument='FB',start='2014-09-19',end='2016-03-31'))
facebook$Date = as.Date(rownames(facebook))
linkedin = as.data.frame(get.hist.quote(instrument='LNKD',start='2014-09-19',end='2016-03-31'))
linkedin$Date = as.Date(rownames(linkedin))
linkedin$PriceDiff = linkedin$Close - linkedin$Open
amazon = as.data.frame(get.hist.quote(instrument='AMZN',start='2014-09-19',end='2016-03-31'))
amazon$Date = as.Date(rownames(amazon))
google = as.data.frame(get.hist.quote(instrument='GOOG',start='2014-09-19',end='2016-03-31'))
google$Date = as.Date(rownames(google))
microsoft = as.data.frame(get.hist.quote(instrument='MSFT',start='2014-09-19',end='2016-03-31'))
microsoft$Date = as.Date(rownames(microsoft))
Alibaba = as.data.frame(get.hist.quote(instrument='BABA',start='2014-09-19',end='2016-03-31'))
Alibaba$Date = as.Date(rownames(Alibaba))
SP_500 = as.data.frame(get.hist.quote(instrument='GSPC',start='2014-09-19',end='2016-03-31'))
SP_500$Date = as.Date(rownames(SP_500))


apple_xts <- xts(apple$Close,order.by=apple$Date,frequency=365)
facebook_xts <- xts(facebook$Close,order.by=facebook$Date,frequency=365)
linkedin_xts <- xts(linkedin$Close,order.by=linkedin$Date,frequency=365)
amazon_xts <- xts(amazon$Close,order.by=amazon$Date,frequency=365)
google_xts <- xts(google$Close,order.by=google$Date,frequency=365)
microsoft_xts <- xts(microsoft$Close,order.by=microsoft$Date,frequency=365)
Alibaba_xts <- xts(Alibaba$Close,order.by=Alibaba$Date,frequency=365)
SP_500_xts <- xts(SP_500$Close,order.by=SP_500$Date,frequency=365)

stocks <- cbind(apple_xts,facebook_xts, linkedin_xts,
                amazon_xts, google_xts, microsoft_xts,
                Alibaba_xts,SP_500_xts)
```


Example: Amazon
For our interested stocks, we have conducted the time series analysis with similar steps to construct a single variable ARIMA model. A detailed exploration process will be shown as below. 

Firstly, we check the ACF and PACF plots for the first-order or second-order difference of the original data. By observing the number of bars outside of the blue lines, which give the values beyond which the autocorrelations are significantly different from zero. 



```{r,echo=FALSE}
pmts=ts(amazon$Close,frequency=1)
par(mfrow=c(1,2))
# acf(pmts,100)
acf(diff(diff(pmts)),100,main="ACF")
pacf(diff(diff(pmts)),100,main="PACF")
```
With this routine step, though there are very much unlikely for the financial data to show a pattern of seasonality, we have still tried to use spectral analysis to detect periodicity of the data by transferring the data from time-domain to frequency domain with the help of FFT(Fast Fourier Transform) and visualize the Raw Periodogram. 
```{r,echo=FALSE}
par(mfrow=c(1,1))
a=spec.pgram(diff(pmts),taper=0,log="no",main="Raw Periodogram")
```

In this specific example, there is an obvious peak when the frequency is about 0.11. Thus a possible seasonality worth testing is about 8 or 9. It is worth noting that the rest of the stocks do not show such pattern. However, taking this into consideration would be an essential step to dig potential information. 

After comparing several SARIMA(Seasonal Autoregressive Integrated Moving Average) models based on their AIC, a best model is selected for making forecast.  Here the best for Amazon is :
$Arima(8,0,2)\times(0,1,0)_{(9)}$

The prediction plot can be seen as below with a shaded area of confidence interval of the forecast. 

```{r,echo=FALSE}
spm0=Arima(pmts,order=c(8,0,2),seasonal=list(order=c(0,1,0),period=9))
#spm0$aic
pmdarmaforecast1=forecast.Arima(spm0,h=21)
par(mfrow=c(2,1), las=1)
plot.forecast(pmdarmaforecast1,ylim=c(250,700),main="Amazon Forecast")

amazon_real = as.data.frame(get.hist.quote(instrument='AMZN',start='2014-09-19',end='2016-4-29'))

plot.ts(amazon_real$Close,ylim=c(250,700),main="Amazon real data")

mse_amazon = round(sum((pmdarmaforecast1$mean[1:20]-amazon_real$Close[380:399])^2)/20,3)

```

From the graph we can see that the SARIMA model shows a similar trend of the data. However, the MSE is `r mse_amazon`.

Following the similar analysis procedure the rest of the forecasts and their MSEs are as below:



```{r,echo=FALSE}
pmts=ts(facebook$Close,frequency=1)
# acf(diff(diff(pmts)),100,main="ACF")
# pacf(diff(diff(pmts)),100,main="PACF")
# spectrum(diff(diff(pmts)))
# a=spec.pgram(diff(diff(pmts)),taper=0,log="no",main="Raw Periodogram")
testarma=Arima(pmts,order=c(9,2,2))
pmdarmaforecast=forecast.Arima(testarma,h=27)
par(mfrow=c(2,2))
plot.forecast(pmdarmaforecast,ylim=c(70,130),main="Facebook Forecast")

facebook_real = as.data.frame(get.hist.quote(instrument='FB',start='2014-09-19',end='2016-4-29'))
plot.ts(facebook_real$Close,ylim=c(70,130),main="Facebook real data")

mse_facebook = sum((pmdarmaforecast$mean[1:20]-facebook_real$Close[380:399])^2)/20





pmts_l=ts(linkedin$Close,frequency=1)
# par(mfrow=c(2,2))
# # acf(pmts_l,100)
# acf(diff(diff(pmts_l)),100,main="ACF")
# pacf(diff(diff(pmts_l)),100,main="PACF")
# spectrum(pmts_l)
# a=spec.pgram(diff(pmts_l),taper=0,log="no",main="Raw Periodogram")

testarma_l=Arima(pmts_l,order=c(7,1,2))
pmdarmaforecast_l=forecast.Arima(testarma_l,h=25)
# par(mfrow=c(2,1))
plot.forecast(pmdarmaforecast_l,main="Linkedin Forecast")

linkedin_real = as.data.frame(get.hist.quote(instrument='LNKD',start='2014-09-19',end='2016-4-29'))

plot.ts(linkedin_real$Close,main="Linkedin real data")

mse_linkedin = sum((pmdarmaforecast_l$mean[1:20]-linkedin_real$Close[380:399])^2)/20






pmts=ts(google$Close,frequency=1)
# par(mfrow=c(2,2))
# # acf(pmts,100)
# acf(diff(diff(pmts)),100)
# pacf(diff(diff(pmts)),100)
# spectrum(pmts)
# a=spec.pgram(diff(pmts),taper=0,log="no",main="Raw Periodogram")
testarma=Arima(pmts,order=c(8,0,1))
pmdarmaforecast=forecast.Arima(testarma,h=25)
par(mfrow=c(2,2))
plot.forecast(pmdarmaforecast,main="Google Forecast")
google_real = as.data.frame(get.hist.quote(instrument='GOOG',start='2014-09-19',end='2016-4-29'))
plot.ts(google_real$Close,main="Google real data")
mse_google = sum((pmdarmaforecast$mean[1:20]-google_real$Close[380:399])^2)/20






pmts=ts(apple$Close,frequency=1)
# par(mfrow=c(2,2))
# # acf(pmts,100)
# acf(diff(diff(pmts)),100)
# pacf(diff(diff(pmts)),100)
# spectrum(pmts)
# a=spec.pgram(diff(pmts),taper=0,log="no",main="Raw Periodogram")
testarma=Arima(pmts,order=c(8,0,2),seasonal=list(order=c(0,1,0),period=3))
pmdarmaforecast=forecast.Arima(testarma,h=25)

plot.forecast(pmdarmaforecast,main="Apple Forecast")
apple_real = as.data.frame(get.hist.quote(instrument='AAPL',start='2014-09-19',end='2016-4-29'))
plot.ts(apple_real$Close,main="Apple real data")
mse_apple = sum((pmdarmaforecast$mean[1:20]-apple_real$Close[380:399])^2)/20






pmts=ts(microsoft$Close,frequency=1)
# par(mfrow=c(2,2))
# # acf(pmts,100)
# acf(diff(diff(pmts)),100)
# pacf(diff(diff(pmts)),100)
# spectrum(pmts)
# a=spec.pgram(diff(pmts),taper=0,log="no",main="Raw Periodogram")
testarma=Arima(pmts,order=c(10,0,1))
pmdarmaforecast=forecast.Arima(testarma,h=25)
par(mfrow=c(2,2))
plot.forecast(pmdarmaforecast,main="Microsoft Forecast")
microsoft_real = as.data.frame(get.hist.quote(instrument='MSFT',start='2014-09-19',end='2016-4-29'))
plot.ts(microsoft_real$Close,main="Microsoft real data")
mse_microsoft = sum((pmdarmaforecast$mean[1:20]-microsoft_real$Close[380:399])^2)/20





pmts=ts(Alibaba$Close,frequency=1)
# par(mfrow=c(2,2))
# # acf(pmts,100)
# acf(diff(diff(pmts)),100)
# pacf(diff(diff(pmts)),100)
# spectrum(pmts)
# a=spec.pgram(diff(pmts),taper=0,log="no",main="Raw Periodogram")

testarma=Arima(pmts,order=c(8,0,2),seasonal=list(order=c(0,1,0),period=3))
pmdarmaforecast=forecast.Arima(testarma,h=25)

plot.forecast(pmdarmaforecast,main="Alibaba Forecast")
Alibaba_real = as.data.frame(get.hist.quote(instrument='BABA',start='2014-09-08',end='2016-4-29'))
plot.ts(Alibaba_real$Close,main="Alibaba real data")
mse_Alibaba = sum((pmdarmaforecast$mean[1:20]-Alibaba_real$Close[380:399])^2)/20
```


Facebook: `r round(mse_facebook,3)`\newline
Linkedin: `r round(mse_linkedin,3)`\newline
Google: `r round(mse_google,3)`\newline
Apple: `r round(mse_apple,3)`\newline
Microsoft: `r round(mse_microsoft,3)`\newline
Alibaba: `r round(mse_Alibaba,3)`\newline



**Neural Network**\newline

Before traning the models, the data is normalized before being input to the ANN. We have guaranteed the input vectors of the training data to have zero-mean and unit variance, while considering the activation function may be Unipolar sigmoidm the target values are normalized between 0 and 1. We could also normalized them to between -1 and 1 and 0 and
$\frac{1}{\sqrt{2\pi}\sigma}$ when the activation function is Bipolar sigmoid, Tan hyperbolic or RBF, but these goes beyond the scope of this report and we stick to the Unipolar sigmoidm activation function for "simplicity". Similarly, the test data vector is also scaled by the same factors with which the training data was normalized. The output values from the ANN for this test vector are also scaled back with the same factor as the target values for the training data and plotted together with the real data for comparison.\newline

In spite of all the features mentioned for neural networks, building a neural network for prediction is somehow complicated. In order to have a satisfactory performance, we must consider some crucial factors in designing of such a prediction model. One of the main factors is the network structure including number of layers, neurons, and the connections. 
In the two proposed models, we followed the empirical rules when choosing the number of layer and neurons and test the model based on error.\newline

In the first model, the inputs to the neural networks are the lowest, the highest and the open values in the d previous days. In the second model, the same variables are taken from the past few d days such that the data can be seen as longitudinal.\newline



1. The first Neural Network model takes the data from a fixed period of time(January and Feburary). The estimated network is shown as below.\newline

![](/Users/YY/Documents/DUKE/Courses/2016Spring/STA531/FinalProject/NN.png)

With this training model, we put in the test data with the same variables for prediction(March and April). The true data is labeled as the black line and the red line is the predicted values. The pattern can be catched very well, but there may be more improvement.

```{r,echo=FALSE}
close1 = tail(amazon$Close,60)
open1 = tail(amazon$Open,60)
high1 = tail(amazon$High,60)
low1 = tail(amazon$Low,60)

close2 = amazon$Close[275:334]
open2 = amazon$Open[275:334]
high2 = amazon$High[275:334]
low2 = amazon$Low[275:334]


test_data <- data.frame(cbind(close1,open1,high1,low1))
colnames(test_data) <- c("close","open","high","low")
train_data <- data.frame(cbind(close2,open2,high2,low2))
colnames(train_data) <- c("close","open","high","low")

#You can choose different methods to scale the data (z-normalization, min-max scale, etcâ€¦).
#I chose to use the min-max method and scale the data in the interval [0,1].
#Usually scaling in the intervals [0,1] or [-1,1] tends to give better results.
maxs <- apply(train_data, 2, max)
mins <- apply(train_data, 2, min)
scaled_train <- as.data.frame(scale(train_data, center = mins, scale = maxs - mins))
maxs <- apply(test_data, 2, max)
mins <- apply(test_data, 2, min)
scaled_test <- as.data.frame(scale(test_data, center = mins, scale = maxs - mins))

#Train nn with scaled training data
set.seed(12)
net_amazon_last <- neuralnet(close ~(open+high+low),
                               scaled_train, err.fct = "sse", hidden = c(5,2),
                            threshold=0.001,linear.output = TRUE)
#plot(net_amazon_last)

net.results_last <- compute(net_amazon_last, scaled_train[,2:4])
pr.nn_last <- net.results_last$net.result
#plot.ts(train_data$lastclose1,ylab="lastclose1")
# par(new=TRUE)
#plot.ts(pr.nn_last,col='red',yaxt="n",ylab="")

net.results_last <- compute(net_amazon_last, scaled_test[,2:4])
pr.nn_last <- net.results_last$net.result
plot.ts(test_data$close,ylab="Amazon Close Price", main="Prediction based on Test data(fixed period)")
# plot.ts(train_data$lastclose1)
par(new=TRUE)
plot.ts(pr.nn_last,col='red',yaxt="n",ylab="")
```

Next, we try to analyze in terms of longitudinal data to improve prediction. The input will be all the variables in the past 3 periods of time (60 days). The best trained network is show as below. 

![](/Users/YY/Documents/DUKE/Courses/2016Spring/STA531/FinalProject/NN1.png)

This time the red and black lines are almost overlapping. Thus by taking into account the data across a relatively long period of time, the prediction accuracy has been improved to a great extent.

```{r,echo=FALSE}
lastclose1 = tail(amazon$Close,60)
lastopen1 = tail(amazon$Open,60)
lasthigh1 = tail(amazon$High,60)
lastlow1 = tail(amazon$Low,60)

lastclose2 = amazon$Close[275:334]
lastopen2 = amazon$Open[275:334]
lasthigh2 = amazon$High[275:334]
lastlow2 = amazon$Low[275:334]

lastclose3 = amazon$Close[215:274]
lastopen3 = amazon$Open[215:274]
lasthigh3 = amazon$High[215:274]
lastlow3 = amazon$Low[215:274]

lastclose4 = amazon$Close[155:214]
lastopen4 = amazon$Open[155:214]
lasthigh4 = amazon$High[155:214]
lastlow4 = amazon$Low[155:214]

lastclose5 = amazon$Close[95:154]
lastopen5 = amazon$Open[95:154]
lasthigh5 = amazon$High[95:154]
lastlow5 = amazon$Low[95:154]


test_data <- data.frame(cbind(lastclose1,lastopen1,lasthigh1,lastlow1,
                               lastclose2,lastopen2,lasthigh2,lastlow2,
                               lastclose3,lastopen3,lasthigh3,lastlow3,
                               lastclose4,lastopen4,lasthigh4,lastlow4
                               ))

train_data <- data.frame(cbind(lastclose2,lastopen2,lasthigh2,lastlow2,
                         lastclose3,lastopen3,lasthigh3,lastlow3,
                         lastclose4,lastopen4,lasthigh4,lastlow4,
                         lastclose5,lastopen5,lasthigh5,lastlow5))
colnames(train_data) <- c("lastclose1","lastopen1","lasthigh1","lastlow1",
                          "lastclose2","lastopen2","lasthigh2","lastlow2",
                          "lastclose3","lastopen3","lasthigh3","lastlow3",
                          "lastclose4","lastopen4","lasthigh4","lastlow4")
#You can choose different methods to scale the data (z-normalization, min-max scale, etcâ€¦).
#I chose to use the min-max method and scale the data in the interval [0,1].
#Usually scaling in the intervals [0,1] or [-1,1] tends to give better results.
maxs <- apply(train_data, 2, max)
mins <- apply(train_data, 2, min)
scaled_train <- as.data.frame(scale(train_data, center = mins, scale = maxs - mins))
maxs <- apply(test_data, 2, max)
mins <- apply(test_data, 2, min)
scaled_test <- as.data.frame(scale(test_data, center = mins, scale = maxs - mins))

#Train nn with scaled training data
set.seed(777)
net_amazon_last <- neuralnet(lastclose1 ~(lastclose2+lastopen2+lasthigh2+lastlow2+
                                        lastclose3+lastopen3+lasthigh3+lastlow3+
                                         lastclose4+lastopen4+lasthigh4+lastlow4),
                               scaled_test, err.fct = "sse", hidden = c(14,3),
                            threshold=0.001,likelihood = TRUE,linear.output = TRUE)

net.results_last <- compute(net_amazon_last, scaled_test[,5:16])
pr.nn_last <- net.results_last$net.result
plot.ts(test_data$lastclose1,ylab="Amazon Close Price", main="Prediction based on past data(longitudinal)")
par(new=TRUE)
plot.ts(pr.nn_last,col='red',yaxt="n",ylab="")
```

```{r,echo=FALSE,eval=FALSE}
net.results_last <- compute(net_amazon_last, scaled_test[,5:16])
pr.nn_last <- net.results_last$net.result
plot.ts(test_data$lastclose1,ylab="lastclose1")
# plot.ts(train_data$lastclose1)
par(new=TRUE)
plot.ts(pr.nn_last,col='red',yaxt="n",ylab="")

# plot(scaled_test$lastclose1,pr.nn_last,col='red',main='Real vs predicted NN',pch=18,cex=0.7)
# abline(0,1,lwd=2)
# legend('bottomright',legend='NN',pch=18,col='red', bty='n')
```


Neural networks (NN) can assist in finding a relationship, or mapping, between inputs (Open, Low and High price) and an output (Closing price). Neural Networks have the advantage that can approximate any nonlinear functions without any apriori information about the properties of the data series. Comparing with traditional methods such as linear regression and logistic regression which are model based, Neural Networks are self-adjusting methods based on training data, so they have the ability to solve the problem with a little knowledge about its model and without constraining the prediction model by adding any extra assumptions. Besides, neural networks can find the relationship between the input and output of the system even if this relationship might be very complicated because they are general function approximators. Consequently, neural networks are well applied to the problems in which extracting the relationships among data is really difficult but on the other hand there exists a large enough training data sets. It should be mentioned that, although sometimes the rules or patterns that we are looking for might not be easily found or the data could be corrupted due to the process or measurement noise of the system, it is still believed that the inductive learning or data driven methods are the best way to deal with real world prediction problems. Furthermore, Neural Networks have generalization ability. After training, they can recognize the new patterns even if they havenâ€™t been in training set. Since in most of the pattern recognition problems predicting future events (unseen data) is based on previous data (training set), the application of neural networks would be very beneficial. 

The application of neural networks in prediction problems is very promising due to some of their special characteristics. But we still need to be clear that the model is mostly strong in explaining about the prediction rather than forecasting. By  implementing neural networks model in this report, it is further proved that including more variables other than merely the Closing price can be helpful. Also, it also shows a strong potential of making forecasting if we are able to get the required variables. Though this may require a complete reconstruction of the data frames and complexity of computation, it can be a promising model worth optimizing. 















Artificial neural network:
Artificial neural network (ANN) is a machine learning model to estimate the function of inputs. It is composed of many neuron-like nodes which have connections with each other.

We construct a model by utilizing the high price, low price, open price, closed price of last 3 time periods to predict all these prices of the next period. With this ANN model, stock price can be predicted by historical data. The inputs are the data of last 3 time periods and the outputs are the stock prices we would like to predict. Here we assume that the relationship between the last 3 time periods and the next time period is roughly stable so that we can utilize the model based on historical data to predict future data.


